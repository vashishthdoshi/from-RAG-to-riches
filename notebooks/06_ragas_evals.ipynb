{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf439c0a-e7c5-4889-a9ae-e857e6aaf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAGAs Evaluation for NaiveRAG and then Enhanced RAG\n",
    "\"\"\"\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Setting OPENAI Key\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_key:\n",
    "    raise ValueError(\"Set OPENAI_API_KEY environment variable\")\n",
    "os.environ['OPENAI_API_KEY'] = openai_key\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Load both datasets\n",
    "with open('../results/ragas_naive_data.json', 'r') as f:\n",
    "\tnaive_data = json.load(f)\n",
    "\n",
    "with open('../results/ragas_enhanced_data.json', 'r') as f:\n",
    "\tenhanced_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded naive: {len(naive_data)} predictions\")\n",
    "print(f\"Loaded enhanced: {len(enhanced_data)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854770c-f3f4-43d5-982e-e5bc90001c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"EVALUATING NAIVE RAG\")\n",
    "print(\"\\n\")\n",
    "\n",
    "naive_dataset = Dataset.from_dict({\n",
    "\t'question': [d['question'] for d in naive_data],\n",
    "\t'answer': [d['answer'] for d in naive_data],\n",
    "\t'contexts': [d['contexts'] for d in naive_data],\n",
    "\t'ground_truth': [d['ground_truth'] for d in naive_data]\n",
    "})\n",
    "\n",
    "naive_results = evaluate(\n",
    "\tnaive_dataset,\n",
    "\tmetrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ed6b0-fdcc-4b9e-85d8-f293042f8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the evaluation scores - calculating means from individual metric per question.\n",
    "#Ignoring nans.\n",
    "import numpy as np\n",
    "\n",
    "naive_scores = {\n",
    "\t'faithfulness': np.nanmean(naive_results['faithfulness']),\n",
    "\t'answer_relevancy': np.nanmean(naive_results['answer_relevancy']),\n",
    "\t'context_precision': np.nanmean(naive_results['context_precision']),\n",
    "\t'context_recall': np.nanmean(naive_results['context_recall'])\n",
    "}\n",
    "\n",
    "print(\"\\nNaive RAG Results:\")\n",
    "for metric, score in naive_scores.items():\n",
    "\tprint(f\"  {metric}: {score:.3f}\")\n",
    "\t\n",
    "# Count how many questions actually evaluated\n",
    "valid_count = sum(1 for x in naive_results['faithfulness'] if not np.isnan(x))\n",
    "print(f\"\\nSuccessfully evaluated: {valid_count}/50 questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86a06d-6553-4c7c-8094-d77f4c6d513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"EVALUATING ENHANCED RAG\")\n",
    "print(\"\\n\")\n",
    "\n",
    "enhanced_dataset = Dataset.from_dict({\n",
    "\t'question': [d['question'] for d in enhanced_data],\n",
    "\t'answer': [d['answer'] for d in enhanced_data],\n",
    "\t'contexts': [d['contexts'] for d in enhanced_data],\n",
    "\t'ground_truth': [d['ground_truth'] for d in enhanced_data]\n",
    "})\n",
    "\n",
    "enhanced_results = evaluate(\n",
    "\tenhanced_dataset,\n",
    "\tmetrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672d9fc-44f1-4aa8-a5b5-c16e5852d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_scores = {\n",
    "\t'faithfulness': np.nanmean(enhanced_results['faithfulness']),\n",
    "\t'answer_relevancy': np.nanmean(enhanced_results['answer_relevancy']),\n",
    "\t'context_precision': np.nanmean(enhanced_results['context_precision']),\n",
    "\t'context_recall': np.nanmean(enhanced_results['context_recall'])\n",
    "}\n",
    "\n",
    "print(\"\\nEnhanced RAG Results:\")\n",
    "for metric, score in enhanced_scores.items():\n",
    "\tprint(f\"  {metric}: {score:.3f}\")\n",
    "\t\n",
    "# Count how many questions actually evaluated\n",
    "valid_count = sum(1 for x in enhanced_results['faithfulness'] if not np.isnan(x))\n",
    "print(f\"\\nSuccessfully evaluated: {valid_count}/50 questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766ae2b-a07b-437c-924d-dbd61919d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "\t'Metric': ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'],\n",
    "\t'Naive RAG': [\n",
    "\t\tnaive_scores['faithfulness'],\n",
    "\t\tnaive_scores['answer_relevancy'],\n",
    "\t\tnaive_scores['context_precision'],\n",
    "\t\tnaive_scores['context_recall']\n",
    "\t],\n",
    "\t'Enhanced RAG': [\n",
    "\t\tenhanced_scores['faithfulness'],\n",
    "\t\tenhanced_scores['answer_relevancy'],\n",
    "\t\tenhanced_scores['context_precision'],\n",
    "\t\tenhanced_scores['context_recall']\n",
    "\t]\n",
    "})\n",
    "\n",
    "comparison['Improvement'] = comparison['Enhanced RAG'] - comparison['Naive RAG']\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"RAGAS COMPARISON\")\n",
    "print(\"\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "print(f\"\\nNote: Naive evaluated on 14 questions, Enhanced on 7 questions\")\n",
    "\n",
    "comparison.to_csv('../results/06_ragas_comparison.csv', index=False)\n",
    "print(\"Results Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3c06a-849d-4f8f-aa4e-83833295bd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ragas-eval]",
   "language": "python",
   "name": "conda-env-ragas-eval-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
