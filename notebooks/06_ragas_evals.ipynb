{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf439c0a-e7c5-4889-a9ae-e857e6aaf304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded naive: 50 predictions\n",
      "Loaded enhanced: 50 predictions\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RAGAs Evaluation for NaiveRAG and then Enhanced RAG\n",
    "\"\"\"\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Setting OPENAI Key\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_key:\n",
    "    raise ValueError(\"Set OPENAI_API_KEY environment variable\")\n",
    "os.environ['OPENAI_API_KEY'] = openai_key\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Load both datasets\n",
    "with open('../results/ragas_naive_data.json', 'r') as f:\n",
    "\tnaive_data = json.load(f)\n",
    "\n",
    "with open('../results/ragas_enhanced_data.json', 'r') as f:\n",
    "\tenhanced_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded naive: {len(naive_data)} predictions\")\n",
    "print(f\"Loaded enhanced: {len(enhanced_data)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b854770c-f3f4-43d5-982e-e5bc90001c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EVALUATING NAIVE RAG\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c3bdab417d403fb3ffffa237569fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[68]: TimeoutError()\n",
      "Exception raised in Job[72]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[106]: TimeoutError()\n",
      "Exception raised in Job[107]: TimeoutError()\n",
      "Exception raised in Job[123]: TimeoutError()\n",
      "Exception raised in Job[127]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[172]: TimeoutError()\n",
      "Exception raised in Job[186]: TimeoutError()\n",
      "Exception raised in Job[188]: TimeoutError()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"EVALUATING NAIVE RAG\")\n",
    "print(\"\\n\")\n",
    "\n",
    "naive_dataset = Dataset.from_dict({\n",
    "\t'question': [d['question'] for d in naive_data],\n",
    "\t'answer': [d['answer'] for d in naive_data],\n",
    "\t'contexts': [d['contexts'] for d in naive_data],\n",
    "\t'ground_truth': [d['ground_truth'] for d in naive_data]\n",
    "})\n",
    "\n",
    "naive_results = evaluate(\n",
    "\tnaive_dataset,\n",
    "\tmetrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3ed6b0-fdcc-4b9e-85d8-f293042f8087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive RAG Results:\n",
      "  faithfulness: 0.693\n",
      "  answer_relevancy: 0.728\n",
      "  context_precision: 0.729\n",
      "  context_recall: 0.617\n",
      "\n",
      "Successfully evaluated: 44/50 questions\n"
     ]
    }
   ],
   "source": [
    "#Extracting the evaluation scores - calculating means from individual metric per question.\n",
    "#Ignoring nans.\n",
    "import numpy as np\n",
    "\n",
    "naive_scores = {\n",
    "\t'faithfulness': np.nanmean(naive_results['faithfulness']),\n",
    "\t'answer_relevancy': np.nanmean(naive_results['answer_relevancy']),\n",
    "\t'context_precision': np.nanmean(naive_results['context_precision']),\n",
    "\t'context_recall': np.nanmean(naive_results['context_recall'])\n",
    "}\n",
    "\n",
    "print(\"\\nNaive RAG Results:\")\n",
    "for metric, score in naive_scores.items():\n",
    "\tprint(f\"  {metric}: {score:.3f}\")\n",
    "\t\n",
    "# Count how many questions actually evaluated\n",
    "valid_count = sum(1 for x in naive_results['faithfulness'] if not np.isnan(x))\n",
    "print(f\"\\nSuccessfully evaluated: {valid_count}/50 questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f86a06d-6553-4c7c-8094-d77f4c6d513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EVALUATING ENHANCED RAG\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c445514b4b2342a295a9c3072d0fca62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[12]: TimeoutError()\n",
      "Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[63]: TimeoutError()\n",
      "Exception raised in Job[86]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[96]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[100]: TimeoutError()\n",
      "Exception raised in Job[104]: TimeoutError()\n",
      "Exception raised in Job[105]: TimeoutError()\n",
      "Exception raised in Job[120]: TimeoutError()\n",
      "Exception raised in Job[129]: TimeoutError()\n",
      "Exception raised in Job[130]: TimeoutError()\n",
      "Exception raised in Job[136]: TimeoutError()\n",
      "Exception raised in Job[147]: TimeoutError()\n",
      "Exception raised in Job[148]: TimeoutError()\n",
      "Exception raised in Job[151]: TimeoutError()\n",
      "Exception raised in Job[153]: TimeoutError()\n",
      "Exception raised in Job[155]: TimeoutError()\n",
      "Exception raised in Job[158]: TimeoutError()\n",
      "Exception raised in Job[159]: TimeoutError()\n",
      "Exception raised in Job[162]: TimeoutError()\n",
      "Exception raised in Job[163]: TimeoutError()\n",
      "Exception raised in Job[166]: TimeoutError()\n",
      "Exception raised in Job[167]: TimeoutError()\n",
      "Exception raised in Job[168]: TimeoutError()\n",
      "Exception raised in Job[169]: TimeoutError()\n",
      "Exception raised in Job[170]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[175]: TimeoutError()\n",
      "Exception raised in Job[179]: TimeoutError()\n",
      "Exception raised in Job[183]: TimeoutError()\n",
      "Exception raised in Job[187]: TimeoutError()\n",
      "Exception raised in Job[199]: TimeoutError()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"EVALUATING ENHANCED RAG\")\n",
    "print(\"\\n\")\n",
    "\n",
    "enhanced_dataset = Dataset.from_dict({\n",
    "\t'question': [d['question'] for d in enhanced_data],\n",
    "\t'answer': [d['answer'] for d in enhanced_data],\n",
    "\t'contexts': [d['contexts'] for d in enhanced_data],\n",
    "\t'ground_truth': [d['ground_truth'] for d in enhanced_data]\n",
    "})\n",
    "\n",
    "enhanced_results = evaluate(\n",
    "\tenhanced_dataset,\n",
    "\tmetrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7672d9fc-44f1-4aa8-a5b5-c16e5852d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced RAG Results:\n",
      "  faithfulness: 0.775\n",
      "  answer_relevancy: 0.719\n",
      "  context_precision: 0.909\n",
      "  context_recall: 0.667\n",
      "\n",
      "Successfully evaluated: 40/50 questions\n"
     ]
    }
   ],
   "source": [
    "enhanced_scores = {\n",
    "\t'faithfulness': np.nanmean(enhanced_results['faithfulness']),\n",
    "\t'answer_relevancy': np.nanmean(enhanced_results['answer_relevancy']),\n",
    "\t'context_precision': np.nanmean(enhanced_results['context_precision']),\n",
    "\t'context_recall': np.nanmean(enhanced_results['context_recall'])\n",
    "}\n",
    "\n",
    "print(\"\\nEnhanced RAG Results:\")\n",
    "for metric, score in enhanced_scores.items():\n",
    "\tprint(f\"  {metric}: {score:.3f}\")\n",
    "\t\n",
    "# Count how many questions actually evaluated\n",
    "valid_count = sum(1 for x in enhanced_results['faithfulness'] if not np.isnan(x))\n",
    "print(f\"\\nSuccessfully evaluated: {valid_count}/50 questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e766ae2b-a07b-437c-924d-dbd61919d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RAGAs COMPARISON\n",
      "\n",
      "\n",
      "           Metric  Naive RAG  Enhanced RAG  Improvement\n",
      "     faithfulness   0.693182      0.775000     0.081818\n",
      " answer_relevancy   0.727969      0.719251    -0.008718\n",
      "context_precision   0.729167      0.909091     0.179924\n",
      "   context_recall   0.617021      0.666667     0.049645\n",
      "\n",
      "Note: Naive evaluated on 44 questions, Enhancement on 40 questions; Random pickings.\n",
      "Results Saved\n"
     ]
    }
   ],
   "source": [
    "comparison = pd.DataFrame({\n",
    "\t'Metric': ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'],\n",
    "\t'Naive RAG': [\n",
    "\t\tnaive_scores['faithfulness'],\n",
    "\t\tnaive_scores['answer_relevancy'],\n",
    "\t\tnaive_scores['context_precision'],\n",
    "\t\tnaive_scores['context_recall']\n",
    "\t],\n",
    "\t'Enhanced RAG': [\n",
    "\t\tenhanced_scores['faithfulness'],\n",
    "\t\tenhanced_scores['answer_relevancy'],\n",
    "\t\tenhanced_scores['context_precision'],\n",
    "\t\tenhanced_scores['context_recall']\n",
    "\t]\n",
    "})\n",
    "\n",
    "comparison['Improvement'] = comparison['Enhanced RAG'] - comparison['Naive RAG']\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"RAGAs COMPARISON\")\n",
    "print(\"\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "print(f\"\\nNote: Naive evaluated on 44 questions, Enhancement on 40 questions; Random pickings.\")\n",
    "\n",
    "comparison.to_csv('../results/06_ragas_comparison.csv', index=False)\n",
    "print(\"Results Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3c06a-849d-4f8f-aa4e-83833295bd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
