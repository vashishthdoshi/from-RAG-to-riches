{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de3dad-6a6a-4242-9a15-a2938e9fd51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:naive_rag:Loading RAG Mini Wikipedia dataset...\n",
      "INFO:naive_rag:Loading text corpus...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from naive_rag import NaiveRAG\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loading the dataset\n",
    "rag = NaiveRAG()\n",
    "documents, qa_pairs = rag.load_dataset()\n",
    "\n",
    "print(\"Dataset Loaded, yes\")\n",
    "print(\"Document Length: \", f\"{len(documents)}\")\n",
    "print(\"QA Pairs: \", f\"{len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a90de-68ca-4169-857c-e35b6bbb8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = [len(doc.split()) for doc in documents]\n",
    "\n",
    "print(\"Document statistics:\")\n",
    "print(f\"Total Documents: {len(documents)}\")\n",
    "print(f\"Average length: {sum(doc_lengths)/ len(doc_lengths):.1f} word\")\n",
    "print(f\"Minimum length: {min(doc_lengths)} words\")\n",
    "print(f\"Maximum length: {max(doc_lengths)} words\")\n",
    "\n",
    "#Visualizing the document courpus' nature\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.hist(doc_lengths, bins = 50, edgecolor = 'black')\n",
    "plt.xlabel('Document Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Document Lengths')\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59c3af-4995-4bde-b520-9dee64816560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample Documents:\\n')\n",
    "for i in range(5):\n",
    "    print(f\"Document {i+1}\")\n",
    "    print(documents[i][:250] + '...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bcb6e9-26c1-4040-bbb3-4726a2a3fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"QA Pair Analysis\")\n",
    "print(f\"\\nTotal Questions: {len(qa_pairs)}\")\n",
    "\n",
    "answer_lengths = [len(qa['answer'].split()) for qa in qa_pairs]\n",
    "print(f\"Answer length - Average: {sum(answer_lengths)/len(answer_lengths):.2f} words\")\n",
    "print(f\"Answer length - Min: {min(answer_lengths)} words\")\n",
    "print(f\"Answer length - Max: {max(answer_lengths)} words\")\n",
    "\n",
    "question_lengths = [len(qa['question'].split()) for qa in qa_pairs]\n",
    "print(f\"Question length - Average: {sum(question_lengths)/len(question_lengths):.2f} words\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Sample QA Pairs\")\n",
    "for i in range(10):\n",
    "    print(f\"\\nQ{i+1}: {qa_pairs[i]['question']}\")\n",
    "    print(f\"A{i+1}: {qa_pairs[i]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a87d5-5865-465d-8f38-d1350199b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Quality Assessment\") #Data quality check\n",
    "empty_docs = [i for i, doc in enumerate(documents) if not doc.strip()] #checking for empty documents\n",
    "print(\"\\nEmpty Documents: \", len(empty_docs))\n",
    "\n",
    "short_docs = [i for i, doc in enumerate(documents) if len(doc.split()) < 10] #checking for very short documents\n",
    "#potential issues can occur due to short lengths\n",
    "print(\"\\nVery short documents - less than 10 words: \", len(short_docs))\n",
    "\n",
    "from collections import Counter #checking answer types and distribution\n",
    "answers_sample = [qa_pairs[i]['answer'] for i in range(min(100, len(qa_pairs)))]\n",
    "answer_types = Counter(answers_sample)\n",
    "print(f\"\\nMost common answer types (sample of 100):\")\n",
    "for answer, count in answer_types.most_common(10):\n",
    "    print(f\" '{answer}' : {count}\")\n",
    "\n",
    "missing_questions = sum(1 for i in range(len(qa_pairs)) if not qa_pairs[i].get('question'))\n",
    "missing_answers = sum(1 for i in range(len(qa_pairs)) if not qa_pairs[i].get('answer'))\n",
    "print(\"Number of missing questions, answers: \", missing_questions, \",\", missing_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f6c92-21fb-405e-b632-cb689fe8ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Key Insights\\n\")\n",
    "\n",
    "print(f\"1. Dataset Scale:\")\n",
    "print(f\"   - {len(documents)} Wikipedia passages\")\n",
    "print(f\"   - {len(qa_pairs)} question-answer pairs\")\n",
    "print(f\"   - Avg document: {sum(doc_lengths)/len(doc_lengths):.0f} words\")\n",
    "\n",
    "print(f\"\\n2. Document Characteristics:\")\n",
    "print(f\"   - Range: {min(doc_lengths)}-{max(doc_lengths)} words\")\n",
    "print(f\"   - All documents are informative paragraphs from Wikipedia\")\n",
    "print(f\"   - Normally structured and factual\")\n",
    "\n",
    "print(f\"\\n3. Q&A Characteristics:\")\n",
    "print(f\"   - Average question length: {sum(question_lengths)/len(question_lengths):.1f} words\")\n",
    "print(f\"   - Answers vary from single words to short phrases\")\n",
    "print(f\"   - Mix of factual, yes/no, and descriptive questions\")\n",
    "\n",
    "print(f\"\\n4. Challenges:\")\n",
    "print(f\"   - Shorter documents may not allow context matching for retrieval.\")\n",
    "print(f\"   - Exact match metrics may nt be reflective of good/ bad implementation - since 'Yes', 'yes' both exist. F1 score will be comparatively more realistic as a metric of good/ bad implementation.\")\n",
    "print(f\"   - Answer length is short - so verbose LLM behaviour should be avoided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc464069-8b0a-4b4b-bf4f-e17780ac21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ac076-94b9-4f31-b050-269b809f2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save basic statistics for documentation\n",
    "import json\n",
    "\n",
    "exploration_stats = {\n",
    "    'total_documents': len(documents),\n",
    "    'total_qa_pairs': len(qa_pairs),\n",
    "    'avg_doc_length': sum(doc_lengths)/len(doc_lengths),\n",
    "    'min_doc_length': min(doc_lengths),\n",
    "    'max_doc_length': max(doc_lengths),\n",
    "    'avg_question_length': sum(question_lengths)/len(question_lengths),\n",
    "    'avg_answer_length': sum(answer_lengths)/len(answer_lengths)\n",
    "}\n",
    "\n",
    "# Create data directory if needed\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "with open('../data/processed/exploration_stats.json', 'w') as f:\n",
    "    json.dump(exploration_stats, f, indent=2)\n",
    "\n",
    "print(\"Exploration statistics saved to data/processed/exploration_stats.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
